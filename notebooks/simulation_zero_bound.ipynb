{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first trial we assume a naive model (LGM) defined as:\n",
    "$$dx_t = \\sigma_t dW_t^{\\mathit{N}}$$\n",
    "\n",
    "Let's define the Numeraire as:\n",
    "$$N(t, x_t) = \\frac{1}{B(0,t)}exp^{H_tx_t + \\frac{1}{2}H_t^2\\zeta_t}$$\n",
    "where $H_t$ and $\\zeta_t$ are known functions.\n",
    "\n",
    "With this let's defined the fundamental equation for the pricing of a derivative under the model. The NPV (Net Present Value) is:\n",
    "$$V_t = V(t, x_t)$$ \n",
    "and the deflated version \n",
    "$$\\overline{V}_t = V(t, x_t) / N(t, x_t)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montecarlo simulation\n",
    "\n",
    "* Brownian path:\n",
    "$$W_t \\sim \\mathcal{N}(0,t)$$\n",
    "$$W[0] = X_0$$\n",
    "$$W[t] = W[t - 1]  + \\mathcal{Z} \\cdot \\Delta t^{\\frac{1}{2}}$$\n",
    "with \n",
    "$$\\mathcal{Z} \\sim \\mathcal{N}(0,1)$$\n",
    "* X:\n",
    "$$X_{t + 1} = X_t + \\sigma \\cdot (W_{t + 1} - W_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.simulator.simulator import MCSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strike value\n",
    "Vt = 2\n",
    "T = 1\n",
    "# Set of parameters\n",
    "T, N_steps, X0, sigma = (T, 100, 0, 0.01)\n",
    "mcsimulator = MCSimulation(T, N_steps, X0, sigma)\n",
    "nsims = int(1e3)\n",
    "test_sims = int(nsims * 0.2)\n",
    "# Training\n",
    "mc_paths, W = mcsimulator.simulate(nsims)\n",
    "mc_paths_transpose = mc_paths.T\n",
    "mc_paths_flatten = mc_paths.flatten('C')\n",
    "w_paths_flatten = W.flatten('C')\n",
    "# Test\n",
    "mc_paths_test, W_test = mcsimulator.simulate(test_sims)\n",
    "mc_paths_test_transpose = mc_paths_test.T\n",
    "mc_paths_test_flatten = mc_paths_test.flatten('C')\n",
    "w_paths_test_flatten = W_test.flatten('C')\n",
    "# Deltas\n",
    "deltaTs = np.linspace(0, T, N_steps)\n",
    "simulations = np.linspace(0, nsims - 1, nsims)\n",
    "simulations = np.int32(np.tile(simulations, N_steps))\n",
    "deltaTs = np.tile(deltaTs.reshape(N_steps, 1), nsims).flatten()\n",
    "df_x = pd.DataFrame(zip(\n",
    "    deltaTs,\n",
    "    mc_paths_flatten,\n",
    "    w_paths_flatten,\n",
    "    simulations\n",
    "), columns = ['dt',\n",
    "              'xt',\n",
    "              'wt',\n",
    "              'simulation'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTs = np.linspace(0, T, N_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "if nsims < 101:\n",
    "    plt.figure(figsize = (15,6))\n",
    "    plt.title('Complete set of paths')\n",
    "    sns.lineplot(x = 'dt', y = 'xt', hue = 'simulation', data = df_x)\n",
    "    plt.show()\n",
    "    plt.figure(figsize = (15,6))\n",
    "    plt.title('Complete paths distribution')\n",
    "    sns.regplot(x = 'dt', y = 'xt', data = df_x, scatter = False)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity with zero bond coupon\n",
    "\n",
    "Numeraire: \n",
    "\n",
    "$$N(t, x_t) = \\frac{1}{D(t)}exp^{H_tx_t + \\frac{1}{2}H_t^2\\zeta_t}$$\n",
    "where:\n",
    "* $D(t)$ given and follows = $D(t) = e^{-rt}$, in this example with $r = 0.03$\n",
    "* $H(t) = \\frac{1 - e^{-\\kappa t}}{\\kappa}$, with $\\kappa = 2$\n",
    "\n",
    "Discount factor (bono cupón 0 que paga 1 en T):\n",
    "\n",
    "$$Z(x_t, t, T) = \\frac{D(T)}{D(t)}exp-((H_T - H_t)x_t - \\frac{1}{2}(H_T^2-H_t^2)\\zeta_t) = \\frac{D(T)}{D(t)}exp(-(H_T - H_t)x_t - \\frac{1}{2}H_T^2\\zeta_t+\\frac{1}{2}H_t^2\\zeta_t) = $$\n",
    "$$\\frac{D(T)}{D(t)}exp(-H_Tx_t + H_tx_t - \\frac{1}{2}H_T^2\\zeta_t+\\frac{1}{2}H_t^2\\zeta_t) = \\frac{D(T)}{D(t)}exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)exp(H_tx_t + \\frac{1}{2}H_t^2\\zeta_t) = $$\n",
    "$$D(T)exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)N(t, x_t)$$\n",
    "where:\n",
    "* $\\zeta(t) = \\int_0^t\\sigma^2(s)ds$, with $\\sigma(s)$ a piecewise function.\n",
    "\n",
    "The sanity aims to retrieve the $D(t)$ after aggregating for each time step $t$ on the previous simulations. Steps:\n",
    "* Calculate $Z(\\cdot)$ for each timestep\n",
    "* Calculate the numeraire $N(\\cdot)$\n",
    "* Get the $\\hat{D}(t)$ for each path and time step as $\\hat{D}(t) = \\frac{Z(\\cdot)}{N(\\cdot)} \\to E[\\hat{D}(t)] = D(t)E[exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)] = D(T)exp(E[-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t]) = D(T)$\n",
    "* Aggregate and compare the value with the theoretical $D(t)$\n",
    "\n",
    "The final objective is to check that $E[-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t] = 0$\n",
    "\n",
    "NOTE: $E[-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t] = -\\frac{1}{2}H_T^2\\zeta_t-H_TE[x_t]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils.utils import FinanceUtils, ZeroBound\n",
    "t_unique = df_x.dt.unique()\n",
    "dict_C = {dt:FinanceUtils.C(dt, sigma_value = sigma) for dt in t_unique}\n",
    "df_x['ct'] = df_x.apply(lambda x: dict_C[x['dt']], axis = 1)\n",
    "xt, t, T, ct = df_x.xt, df_x.dt, np.float32(T), df_x.ct\n",
    "df_x['d_hat_t'] = ZeroBound.Z(xt, t, T, ct) / ZeroBound.N(t, xt, ct)\n",
    "df_x['d_hat_tensor_t'] = ZeroBound.Z_tensor(xt, t, T, ct) / ZeroBound.N_tensor(t, xt, ct)\n",
    "df_x['exponent'] = ZeroBound.exponent(xt, t, T, ct)\n",
    "sanity = df_x.groupby(['dt']).agg(\n",
    "    d_hat_t = ('d_hat_t', 'mean'),\n",
    "    d_hat_tensor_t = ('d_hat_tensor_t','mean')\n",
    ").reset_index()\n",
    "sanity['dts'] = ZeroBound.D(np.array(sorted(t_unique)))\n",
    "print(f'Expected exponent: {df_x.exponent.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_unique = df_x.dt.unique()\n",
    "dict_C = {dt:FinanceUtils.C(dt, sigma_value = sigma) for dt in t_unique}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize = (10,3))\n",
    "plt.title('Sanity discount factor')\n",
    "sns.lineplot(x = 'dt', y = 'd_hat_t', data = sanity, color = 'red')\n",
    "sns.lineplot(x = 'dt', y = 'd_hat_tensor_t', data = sanity, color = 'green')\n",
    "sns.lineplot(x = 'dt', y = 'dts', data = sanity, color = 'blue')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,3))\n",
    "plt.title('Sanity discount factor')\n",
    "for r in np.arange(0.01, 1.0, step = 0.01):\n",
    "    sns.lineplot(x = t_unique, y = ZeroBound.D(np.array(sorted(t_unique)), r = r))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq with feed forward neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "* https://towardsdatascience.com/how-to-use-custom-losses-with-custom-gradients-in-tensorflow-with-keras-e87f19d13bd5\n",
    "* https://www.tensorflow.org/guide/autodiff\n",
    "\n",
    "Current approaches:\n",
    "* Traditional feed forward with two features as input features:\n",
    "    * x_t - brownian path simulated\n",
    "    * t - temporal instant \n",
    "    * The problem I see here is that we lose the $\\frac{\\delta \\overline{V}_i}{\\delta x_i}$, since the network is unique, thus we have: $\\frac{\\delta \\overline{V}}{\\delta x}(x_i)$\n",
    "* Sequence models based:\n",
    "    * The good part of this approach is $\\mathcal{J} = (\\nabla f_1,\\dots, \\nabla f_T)$ is a diagonal matrix. Since $f_i$ does not depend on $x_{j}, i \\neq j$. Therefore, getting the gradients from the NN is much easier.\n",
    "\n",
    "The idea is to include in the loss function the gradient tape to respect the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.backend.clear_session() \n",
    "keras.backend.set_floatx('float64')\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative process\n",
    "* F - neural network function.\n",
    "* $\\frac{\\delta F}{\\delta X_t}^i$ - gradient calculated by using the model at $i$-iteration.\n",
    "* $\\phi(n, x_n)$ - known terminal function.\n",
    "\n",
    "\n",
    "**Path generation**\n",
    "\n",
    "Theoretical:\n",
    "$$\\overline{V}_t = V(t, x_t) / N(t, x_t)$$\n",
    "$$\\overline{V}_{t+1} = \\overline{V}_t + \\frac{\\delta V(t)}{\\delta x_t}(x_{t + 1} - x_{t})$$\n",
    "with $\\phi(x_T, T) = \\overline{V}_T$ known.\n",
    "\n",
    "With NN\n",
    "$$\\hat{V} = F(X)$$\n",
    "\n",
    "$$\\overline{V}_{t+1} = Pred_t + \\frac{\\delta F(X)}{\\delta x_t}(x_{t + 1} - x_{t})$$\n",
    "\n",
    "**Loss function**\n",
    "$$\\mathcal{L}(\\overline{V}) = \\beta_1 \\cdot (\\overline{V}_T - \\phi(T, x_T))^2 + \\beta_2\\cdot (\\frac{\\delta NN_T}{\\delta x_T} - \\frac{\\delta \\phi(T, x_T)}{\\delta x_T})^2 + \\beta_3\\sum_{i = 1}^{T - 1}(\\overline{V}_i - Pred_i)^2$$\n",
    "\n",
    "This loss function has a problem when the architecture is single step at a time, that is $F(x,t) \\in R^{2,1}$. The problem is that for each path of length $N$ produce only one single error loss. Nevertheless, we can overcome this by defining the error as a triangular inferior matrix per sample as:\n",
    "\n",
    "Monte Carlo (Deep RL) approach:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{stepsLoss} = \\beta_3\\cdot\\begin{pmatrix}\n",
    " 0. \\\\\n",
    " 0. \\\\ \n",
    " \\dots \\\\\n",
    " \\dots \\\\ \n",
    " \\sum_{i = 0}^{N-1}(\\overline{V}_{i} - Pred_{i})^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{stepsLoss} = \\beta_3\\cdot\\begin{pmatrix}\n",
    "(\\overline{V}_{0} - Pred_{0})^2& \\dots & 0 & 0 \\\\\n",
    "(\\overline{V}_{0} - Pred_{0})^2 & \\sum_{i = 0}^{2}(\\overline{V}_{i} - Pred_{i})^2 & \\dots & 0 \\\\\n",
    " & & \\dots & \\\\\n",
    "(\\overline{V}_{0} - Pred_{0})^2 & \\sum_{i = 0}^{2}(\\overline{V}_{i} - Pred_{i})^2 & \\dots & \\sum_{i = 0}^{N-1}(\\overline{V}_{i} - Pred_{i})^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "After taking the `diag`\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{stepsLoss} = \\beta_3\\cdot\\begin{pmatrix}\n",
    "(\\overline{V}_{0} - Pred_{0})^2 \\\\\n",
    " \\sum_{i = 0}^{2}(\\overline{V}_{i} - Pred_{i})^2\\\\ \n",
    " \\dots\\\\\n",
    " \\dots \\\\ \\sum_{i = 0}^{N-1}(\\overline{V}_{i} - Pred_{i})^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Option B:\n",
    "$$\n",
    "\\mathcal{L}_{stepsLoss} = \\beta_3\\cdot\\begin{pmatrix}\n",
    "(\\overline{V}_{0} - Pred_{0})^2 \\\\\n",
    " (\\overline{V}_{1} - Pred_{1})^2\\\\ \n",
    " \\dots\\\\\n",
    " \\dots \\\\ (\\overline{V}_{N} - Pred_{N})^2\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phi(\\cdot)$:\n",
    "* Constant function $f(x, n) = 1$\n",
    "* IRS - TODO: Check\n",
    "\n",
    "Experiments:\n",
    "* Terminal condition 1 - static case:\n",
    "    * Train the network and do the sanity against the payoff for a Zero Bound Coupon $\\overline{V}(x_t,t) = \\frac{Z(t, x_t, T)}{N(x_t, t)}$\n",
    "    * Second detail: The NN (predict vectorized) predicts the value $\\overline{V}(x_t, t)$ which is our objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single step model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils.utils import MLUtils\n",
    "dts = list(np.linspace(0, T, N_steps)) * len(mc_paths_transpose)\n",
    "simulation = [\n",
    "    [i] * N_steps for i in range(nsims)\n",
    "]\n",
    "df_x_tmp = pd.DataFrame(\n",
    "    zip(\n",
    "        mc_paths_transpose,\n",
    "        simulation\n",
    "    ),\n",
    "    columns = [\n",
    "        'path',\n",
    "        'simulation'\n",
    "    ]\n",
    ")\n",
    "df_x = pd.DataFrame()\n",
    "df_x['xt'] = df_x_tmp.explode('path')['path']\n",
    "df_x['dt'] = dts\n",
    "df_x['simulation'] = df_x_tmp.explode('simulation')['simulation']\n",
    "df_x['_delta_x'] = df_x.groupby([\n",
    "    'simulation',\n",
    "])['xt'].shift(1)\n",
    "df_x['_delta_x'] = (df_x['xt'] - df_x['_delta_x'])\n",
    "df_x.loc[df_x._delta_x.isna(), '_delta_x'] = 0.\n",
    "# Sort to get the examples in blocks\n",
    "df_x.sort_values(\n",
    "    [\n",
    "        'simulation',\n",
    "        'dt'\n",
    "    ],\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from model.model_lgm_single_step import LGM_model_one_step\n",
    "\n",
    "epochs = 50\n",
    "mc_paths_tranformed = df_x[['xt', 'dt']].values\n",
    "# Data used as features\n",
    "x = mc_paths_tranformed.astype(np.float64)\n",
    "delta_x = df_x._delta_x.values.astype(np.float64)\n",
    "print(f'Features shape: {x.shape}')\n",
    "# Batch execution with baby steps\n",
    "size_of_the_batch = 100\n",
    "batch_size = size_of_the_batch * N_steps\n",
    "batches = int(np.floor(nsims * N_steps / batch_size))\n",
    "print(f'Batch size: {size_of_the_batch} paths per epoch with length {N_steps}')\n",
    "# Custom model\n",
    "lgm_single_step = LGM_model_one_step(n_steps = N_steps, \n",
    "                                     T = T, \n",
    "                                     verbose = False,\n",
    "                                     sigma = sigma,\n",
    "                                     batch_size = size_of_the_batch,\n",
    "                                     phi = ZeroBound.Z_normalized,\n",
    "                                     future_T = T)\n",
    "print(f'{lgm_single_step.summary()}')\n",
    "# Compile the model\n",
    "lgm_single_step.define_compiler(optimizer = 'adam', learning_rate = 1e-3)\n",
    "# Losses:\n",
    "losses_split = {\n",
    "    'total': [],\n",
    "    'strike_loss': [],\n",
    "    'strike_derivative_loss': [],\n",
    "    'steps_error': []\n",
    "}\n",
    "# Custom iteration: \n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(f'{epoch}...', end = '')\n",
    "    dimension = (1, batches)\n",
    "    total_tmp, strike_tmp, derivative_tmp, steps_tmp = (\n",
    "        np.zeros(dimension),\n",
    "        np.zeros(dimension),\n",
    "        np.zeros(dimension),\n",
    "        np.zeros(dimension)\n",
    "    )\n",
    "    for batch in range(batches):\n",
    "        x_batch = x[batch * batch_size: (batch + 1) * batch_size, :]\n",
    "        delta_x_batch = delta_x[batch * batch_size: (batch + 1) * batch_size]\n",
    "        el, sl, sdl, se = lgm_single_step.custom_train_step(\n",
    "            X = x_batch,\n",
    "            batch = batch,\n",
    "            epoch = epoch, \n",
    "            start_time = start_time,\n",
    "            delta_x = delta_x_batch)\n",
    "        # Store partial results\n",
    "        total_tmp[0, batch] = el\n",
    "        strike_tmp[0, batch] = sl\n",
    "        derivative_tmp[0, batch] = sdl\n",
    "        steps_tmp[0, batch] = se\n",
    "    losses_split['total'].append(\n",
    "        el\n",
    "    )\n",
    "    losses_split['strike_loss'].append(\n",
    "        sl\n",
    "    )\n",
    "    losses_split['strike_derivative_loss'].append(\n",
    "        sdl\n",
    "    )\n",
    "    losses_split['steps_error'].append(\n",
    "        se\n",
    "    )\n",
    "# Visualize errors\n",
    "plt.figure()\n",
    "sns.lineplot(x = [i for i in range(epochs)], \n",
    "             y = losses_split['total'], \n",
    "             color = 'blue')\n",
    "sns.lineplot(x = [i for i in range(epochs)], \n",
    "             y = losses_split['strike_loss'],\n",
    "             color = 'red')\n",
    "sns.lineplot(x = [i for i in range(epochs)], \n",
    "             y = losses_split['strike_derivative_loss'],\n",
    "             color = 'orange')\n",
    "sns.lineplot(x = [i for i in range(epochs)], \n",
    "             y = losses_split['steps_error'],\n",
    "             color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity the NN\n",
    "\n",
    "Once again we use the analytical solution for a ***Zero Coupon Bond***:\n",
    "\n",
    "Numeraire: \n",
    "\n",
    "$$N(t, x_t) = \\frac{1}{D(t)}exp^{H_tx_t + \\frac{1}{2}H_t^2\\zeta_t}$$\n",
    "where:\n",
    "* $D(t)$ given and follows = $D(t) = e^{-rt}$, in this example with $r = 0.03$\n",
    "* $H(t) = \\frac{1 - e^{-\\kappa t}}{\\kappa}$, with $\\kappa = 2$\n",
    "\n",
    "Discount factor (bono cupón 0 que paga 1 en T):\n",
    "\n",
    "$$Z(x_t, t, T) = \\frac{D(T)}{D(t)}exp-((H_T - H_t)x_t - \\frac{1}{2}(H_T^2-H_t^2)\\zeta_t) = \\frac{D(T)}{D(t)}exp(-(H_T - H_t)x_t - \\frac{1}{2}H_T^2\\zeta_t+\\frac{1}{2}H_t^2\\zeta_t) = $$\n",
    "$$\\frac{D(T)}{D(t)}exp(-H_Tx_t + H_tx_t - \\frac{1}{2}H_T^2\\zeta_t+\\frac{1}{2}H_t^2\\zeta_t) = \\frac{D(T)}{D(t)}exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)exp(H_tx_t + \\frac{1}{2}H_t^2\\zeta_t) = $$\n",
    "$$D(T)exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)N(t, x_t)$$\n",
    "where:\n",
    "* $\\zeta(t) = \\int_0^t\\sigma^2(s)ds$, with $\\sigma(s)$ a piecewise function.\n",
    "\n",
    "But we are estimating $\\overline{V}_t = \\frac{Z(x_t, t, T)}{N(t, x_t)} = D(T)exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Single step network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = list(np.linspace(0, T, N_steps)) * len(mc_paths_test_transpose)\n",
    "simulation = [\n",
    "    [i] * N_steps for i in range(test_sims)\n",
    "]\n",
    "df_x_tmp = pd.DataFrame(\n",
    "    zip(\n",
    "        mc_paths_test_transpose,\n",
    "        simulation\n",
    "    ),\n",
    "    columns = [\n",
    "        'path',\n",
    "        'simulation'\n",
    "    ]\n",
    ")\n",
    "df_x = pd.DataFrame()\n",
    "df_x['xt'] = df_x_tmp.explode('path')['path']\n",
    "df_x['dt'] = dts\n",
    "df_x['simulation'] = df_x_tmp.explode('simulation')['simulation']\n",
    "df_x['_delta_x'] = df_x.groupby([\n",
    "    'simulation',\n",
    "])['xt'].shift(1)\n",
    "df_x['_delta_x'] = (df_x['xt'] - df_x['_delta_x'])\n",
    "df_x.loc[df_x._delta_x.isna(), '_delta_x'] = 0.\n",
    "# Sort to get the examples in blocks\n",
    "df_x.sort_values(\n",
    "    [\n",
    "        'simulation',\n",
    "        'dt'\n",
    "    ],\n",
    "    inplace = True\n",
    ")\n",
    "# Adjust x\n",
    "mc_paths_tranformed = df_x[['xt', 'dt']].values.astype(np.float64)\n",
    "delta_x = df_x._delta_x.values.astype(np.float64)\n",
    "# mc_paths_tranformed = np.reshape(mc_paths_tranformed, (N_steps * nsims, 2)).T\n",
    "# Data used as features\n",
    "x = mc_paths_tranformed.astype(np.float64)\n",
    "x = tf.constant(x)\n",
    "v_lgm_single_step, predictions = lgm_single_step.predict(x, \n",
    "                                                         delta_x,\n",
    "                                                         build_masks = True, \n",
    "                                                         debug = True)\n",
    "# Adapt output\n",
    "results = pd.DataFrame(\n",
    "    zip(v_lgm_single_step),\n",
    "    columns = ['results']\n",
    ")\n",
    "v_lgm_single_step = results.explode('results').values\n",
    "df_x['lgm_single_step_V'] = v_lgm_single_step.astype(np.float64)\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df = pd.DataFrame(\n",
    "    zip(df_x.lgm_single_step_V.values,\n",
    "        df_x.dt.values, \n",
    "        df_x.xt.values,\n",
    "        df_x.simulation.values), \n",
    "    columns = ['V_hat_single_step', \n",
    "               'deltat', \n",
    "               'xt',\n",
    "               'simulation'])\n",
    "t_unique = v_df.deltat.unique()\n",
    "dict_C = {dt:FinanceUtils.C(dt, sigma_value = sigma) for dt in t_unique}\n",
    "v_df['ct'] = v_df.apply(lambda x: dict_C[x['deltat']], axis = 1)\n",
    "v_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z value\n",
    "print(f'Calculating V')\n",
    "v_df['Z'] = v_df.apply(\n",
    "    lambda x: \n",
    "        ZeroBound.Z(x.xt, x.deltat, T, x.ct), \n",
    "        axis = 1)\n",
    "# Getting N\n",
    "print(f'Getting N')\n",
    "v_df['N'] = v_df.apply(\n",
    "    lambda x: \n",
    "        ZeroBound.N(x.deltat, x.xt, x.ct), \n",
    "        axis = 1)\n",
    "# Getting V\n",
    "v_df['V'] = v_df.Z \n",
    "v_df['V_adjusted'] = v_df.V_hat_single_step * v_df.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df[['N', 'Z', 'V', 'V_hat_single_step', 'V_adjusted', 'simulation', 'deltat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (v_df.deltat == T)\n",
    "v_strike_loss = v_df.loc[\n",
    "    condition\n",
    "]\n",
    "print(f'Strike error: {np.sqrt(np.mean((v_strike_loss.V - v_strike_loss.V_hat_single_step)**2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_strike_loss[['V', 'V_hat_single_step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.mean(np.abs(v_df.V_hat_single_step - v_df.V))\n",
    "print(f'Absolute error: {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_predicted = v_df.V_hat_single_step.astype(np.float64).values.reshape((test_sims, N_steps))\n",
    "trajectory_real = v_df.V.values.reshape((test_sims, N_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize = (5, 5))\n",
    "sns.lineplot(\n",
    "    x = 'deltat',\n",
    "    y = 'V',\n",
    "    color = 'red',\n",
    "    hue = 'simulation',\n",
    "    data = v_df\n",
    ")\n",
    "sns.lineplot(\n",
    "    x = 'deltat',\n",
    "    y = 'V_hat_single_step',\n",
    "    color = 'blue',\n",
    "    hue = 'simulation',\n",
    "    data = v_df\n",
    ")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b391e97e35ec4987120a2f780cd64183fdb56c026e5a7e01d3347b3d6528b2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
