{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first trial we assume a naive model (LGM) defined as:\n",
    "$$dx_t = \\sigma_t dW_t^{\\mathit{N}}$$\n",
    "\n",
    "Let's define the Numeraire as:\n",
    "$$N(t, x_t) = \\frac{1}{B(0,t)}exp^{H_tx_t + \\frac{1}{2}H_t^2\\zeta_t}$$\n",
    "where $H_t$ and $\\zeta_t$ are known functions.\n",
    "\n",
    "With this let's defined the fundamental equation for the pricing of a derivative under the model. The NPV (Net Present Value) is:\n",
    "$$V_t = V(t, x_t)$$ \n",
    "and the deflated version \n",
    "$$\\overline{V}_t = V(t, x_t) / N(t, x_t)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montecarlo simulation\n",
    "\n",
    "* Brownian path:\n",
    "$$W_t \\sim \\mathcal{N}(0,t)$$\n",
    "$$W[0] = X_0$$\n",
    "$$W[t] = W[t - 1]  + \\mathcal{Z} \\cdot \\Delta t^{\\frac{1}{2}}$$\n",
    "with \n",
    "$$\\mathcal{Z} \\sim \\mathcal{N}(0,1)$$\n",
    "* X:\n",
    "$$X_{t + 1} = X_t + \\sigma \\cdot (W_{t + 1} - W_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "\n",
    "class MCSimulation():\n",
    "    def __init__(self, T, N, X0, sigma, model = 'LGM'):\n",
    "        self._params = {\n",
    "            'T':T,\n",
    "            'N':N,\n",
    "            'X0':X0,\n",
    "            'sigma':sigma,\n",
    "        }\n",
    "        self._model = model\n",
    "        \n",
    "    def simulate(self, nsim = 1e3, show = False):\n",
    "        T, N, X0, sigma = itemgetter('T', 'N', 'X0', 'sigma')(self._params)\n",
    "        nsim = int(nsim)\n",
    "        dt = T / N\n",
    "        # Brownian simulation\n",
    "        W, X = np.zeros([N,nsim]), np.zeros([N,nsim])\n",
    "        # Starting point\n",
    "        W[0] = X0\n",
    "        X[0] = X0\n",
    "        for i in range(1, N):\n",
    "            W[i] = W[i - 1] + np.random.randn(nsim) * np.sqrt(dt)\n",
    "        # X simulation\n",
    "        for i in range(1, N):\n",
    "            X[i] = X[i - 1] + sigma * (W[i] - W[i - 1])\n",
    "        if show:\n",
    "            X = np.linspace(0, T, N)\n",
    "        \n",
    "        return X, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m mcsimulator \u001b[38;5;241m=\u001b[39m MCSimulation(T, N, X0, sigma)\n\u001b[1;32m      7\u001b[0m nsims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e3\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m mc_paths, W \u001b[38;5;241m=\u001b[39m mcsimulator\u001b[38;5;241m.\u001b[39msimulate(nsims)\n\u001b[1;32m      9\u001b[0m mc_paths_flatten \u001b[38;5;241m=\u001b[39m mc_paths\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m deltaTs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, T, N)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Strike value\n",
    "Vt = 2\n",
    "T = 1\n",
    "# Set of parameters\n",
    "T, N, X0, sigma = (T, 100, 0, 0.0075)\n",
    "mcsimulator = MCSimulation(T, N, X0, sigma)\n",
    "nsims = int(1e3)\n",
    "mc_paths, W = mcsimulator.simulate(nsims)\n",
    "mc_paths_flatten = mc_paths.flatten('C')\n",
    "deltaTs = np.linspace(0, T, N)\n",
    "deltaTs = np.tile(deltaTs.reshape(N, 1), nsims).flatten()\n",
    "df_x = pd.DataFrame(zip(\n",
    "    deltaTs,\n",
    "    mc_paths_flatten\n",
    "), columns = ['dt', 'xt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08695652, 0.17391304, 0.26086957, 0.34782609,\n",
       "       0.43478261, 0.52173913, 0.60869565, 0.69565217, 0.7826087 ,\n",
       "       0.86956522, 0.95652174, 1.04347826, 1.13043478, 1.2173913 ,\n",
       "       1.30434783, 1.39130435, 1.47826087, 1.56521739, 1.65217391,\n",
       "       1.73913043, 1.82608696, 1.91304348, 2.        ])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTs = np.linspace(0, 2, 24)\n",
    "deltaTs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_paths_transpose = mc_paths.T\n",
    "deltaTs = np.linspace(0, T, N)\n",
    "mc_value_per_time_step = np.mean(mc_paths_transpose, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nsims < 101:\n",
    "    plt.figure(figsize = (15,6))\n",
    "    plt.title('Complete set of paths')\n",
    "    for vect in mc_paths_transpose:\n",
    "        sns.lineplot(x = deltaTs, y = vect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity with zero bond coupon\n",
    "\n",
    "Numeraire: \n",
    "\n",
    "$$N(t, x_t) = \\frac{1}{D(t)}exp^{H_tx_t + \\frac{1}{2}H_t^2\\zeta_t}$$\n",
    "where:\n",
    "* $D(t)$ given and follows = $D(t) = e^{-rt}$, in this example with $r = 0.03$\n",
    "* $H(t) = \\frac{1 - e^{-\\kappa t}}{\\kappa}$, with $\\kappa = 2$\n",
    "\n",
    "Discount factor:\n",
    "\n",
    "$$Z(x_t, t, T) = \\frac{D(T)}{D(t)}exp-((H_T - H_t)x_t - \\frac{1}{2}(H_T^2-H_t^2)\\zeta_t) = \\frac{D(T)}{D(t)}exp(-(H_T - H_t)x_t - \\frac{1}{2}H_T^2\\zeta_t+\\frac{1}{2}H_t^2\\zeta_t) = $$\n",
    "$$\\frac{D(T)}{D(t)}exp(-H_Tx_t + H_tx_t - \\frac{1}{2}H_T^2\\zeta_t+\\frac{1}{2}H_t^2\\zeta_t) = \\frac{D(T)}{D(t)}exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)exp(H_tx_t + \\frac{1}{2}H_t^2\\zeta_t) = $$\n",
    "$$D(T)exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)N(t, x_t)$$\n",
    "where:\n",
    "* $\\zeta(t) = \\int_0^t\\sigma^2(s)ds$, with $\\sigma(s)$ a piecewise function.\n",
    "\n",
    "The sanity aims to retrieve the $D(t)$ after aggregating for each time step $t$ on the previous simulations. Steps:\n",
    "* Calculate $Z(\\cdot)$ for each timestep\n",
    "* Calculate the numeraire $N(\\cdot)$\n",
    "* Get the $\\hat{D}(t)$ for each path and time step as $\\hat{D}(t) = \\frac{Z(\\cdot)}{N(\\cdot)} \\to E[\\hat{D}(t)] = D(t)E[exp(-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t)] = D(T)exp(E[-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t]) = D(T)$\n",
    "* Aggregate and compare the value with the theoretical $D(t)$\n",
    "\n",
    "The final objective is to check that $E[-\\frac{1}{2}H_T^2\\zeta_t-H_Tx_t] = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount factor\n",
    "def D(t, r = 0.03):\n",
    "    return np.exp(-r * t)\n",
    "# H\n",
    "def H(t, kappa = 2):\n",
    "    return (1 - np.exp(-kappa * t)) / kappa\n",
    "# Volatility function\n",
    "def sigma(t, sigma_0 = 0.75):\n",
    "    return sigma_0 + (t // 2) * 0.05\n",
    "# Zeta\n",
    "def C(t, sigma = sigma):\n",
    "    import scipy.integrate as integrate\n",
    "    return integrate.quad(lambda x: sigma(x)**2, 0, t)[0]    \n",
    "# Numeraire\n",
    "def N(t, xt, ct):\n",
    "    return 1/D(t) * np.exp(H(t) * xt + 0.5 * H(t)**2 * ct)\n",
    "# Factor de descuento a tiempo t dado\n",
    "def Z(xt, t, T, ct = None):\n",
    "    assert ct is not None\n",
    "    return D(T) * np.exp(-0.5 * H(T)**2 * ct - H(T)*xt) * N(t, xt, ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>xt</th>\n",
       "      <th>ct</th>\n",
       "      <th>d_hat_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dt   xt   ct   d_hat_t\n",
       "0    0.0  0.0  0.0  0.941765\n",
       "658  0.0  0.0  0.0  0.941765\n",
       "659  0.0  0.0  0.0  0.941765\n",
       "660  0.0  0.0  0.0  0.941765\n",
       "661  0.0  0.0  0.0  0.941765\n",
       "662  0.0  0.0  0.0  0.941765\n",
       "663  0.0  0.0  0.0  0.941765\n",
       "664  0.0  0.0  0.0  0.941765\n",
       "665  0.0  0.0  0.0  0.941765\n",
       "666  0.0  0.0  0.0  0.941765"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.sort_values(['dt']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.941765\n",
       "1        0.941765\n",
       "2        0.941765\n",
       "3        0.941765\n",
       "4        0.941765\n",
       "           ...   \n",
       "23995    0.714399\n",
       "23996    0.703209\n",
       "23997    0.727597\n",
       "23998    0.717490\n",
       "23999    0.711322\n",
       "Name: xt, Length: 24000, dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D(T) * np.exp(-0.5 * H(T) * df_x.ct.values - H(T) * df_x.xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_unique = df_x.dt.unique()\n",
    "dict_C = {dt:C(dt) for dt in t_unique}\n",
    "df_x['ct'] = df_x.apply(lambda x: dict_C[x['dt']], axis = 1)\n",
    "xt, t, T, ct = df_x.xt, df_x.dt, T, df_x.ct\n",
    "df_x['d_hat_t'] = Z(xt, t, T, ct) / N(t, xt, ct)\n",
    "sanity = df_x.groupby(['dt']).agg(\n",
    "    d_hat_t = ('d_hat_t', 'mean')\n",
    ").reset_index()\n",
    "sanity['dts'] = D(np.array(sorted(t_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>d_hat_t</th>\n",
       "      <th>dts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.941765</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.936234</td>\n",
       "      <td>0.997395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.930666</td>\n",
       "      <td>0.994796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.925173</td>\n",
       "      <td>0.992204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.919584</td>\n",
       "      <td>0.989619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.914244</td>\n",
       "      <td>0.987041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.908836</td>\n",
       "      <td>0.984470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.903426</td>\n",
       "      <td>0.981905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.898161</td>\n",
       "      <td>0.979347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.893036</td>\n",
       "      <td>0.976795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.887793</td>\n",
       "      <td>0.974250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.882646</td>\n",
       "      <td>0.971712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.043478</td>\n",
       "      <td>0.877414</td>\n",
       "      <td>0.969181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.130435</td>\n",
       "      <td>0.872163</td>\n",
       "      <td>0.966656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.217391</td>\n",
       "      <td>0.867057</td>\n",
       "      <td>0.964137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.304348</td>\n",
       "      <td>0.861890</td>\n",
       "      <td>0.961625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.391304</td>\n",
       "      <td>0.857011</td>\n",
       "      <td>0.959120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.478261</td>\n",
       "      <td>0.851860</td>\n",
       "      <td>0.956621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.565217</td>\n",
       "      <td>0.847040</td>\n",
       "      <td>0.954129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.652174</td>\n",
       "      <td>0.842104</td>\n",
       "      <td>0.951643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.739130</td>\n",
       "      <td>0.837031</td>\n",
       "      <td>0.949164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.826087</td>\n",
       "      <td>0.832018</td>\n",
       "      <td>0.946691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.913043</td>\n",
       "      <td>0.827227</td>\n",
       "      <td>0.944225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.822471</td>\n",
       "      <td>0.941765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt   d_hat_t       dts\n",
       "0   0.000000  0.941765  1.000000\n",
       "1   0.086957  0.936234  0.997395\n",
       "2   0.173913  0.930666  0.994796\n",
       "3   0.260870  0.925173  0.992204\n",
       "4   0.347826  0.919584  0.989619\n",
       "5   0.434783  0.914244  0.987041\n",
       "6   0.521739  0.908836  0.984470\n",
       "7   0.608696  0.903426  0.981905\n",
       "8   0.695652  0.898161  0.979347\n",
       "9   0.782609  0.893036  0.976795\n",
       "10  0.869565  0.887793  0.974250\n",
       "11  0.956522  0.882646  0.971712\n",
       "12  1.043478  0.877414  0.969181\n",
       "13  1.130435  0.872163  0.966656\n",
       "14  1.217391  0.867057  0.964137\n",
       "15  1.304348  0.861890  0.961625\n",
       "16  1.391304  0.857011  0.959120\n",
       "17  1.478261  0.851860  0.956621\n",
       "18  1.565217  0.847040  0.954129\n",
       "19  1.652174  0.842104  0.951643\n",
       "20  1.739130  0.837031  0.949164\n",
       "21  1.826087  0.832018  0.946691\n",
       "22  1.913043  0.827227  0.944225\n",
       "23  2.000000  0.822471  0.941765"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity.head(120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq with feed forward neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "* https://towardsdatascience.com/how-to-use-custom-losses-with-custom-gradients-in-tensorflow-with-keras-e87f19d13bd5\n",
    "* https://www.tensorflow.org/guide/autodiff\n",
    "\n",
    "The idea is to include in the loss function the gradient tape to respect the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() \n",
    "keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGM_model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_steps,\n",
    "        intermediate_dim=64,\n",
    "        is_sequential = False,\n",
    "        name=\"LGM_NN_model\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(LGM_model, self).__init__(name=name, **kwargs)\n",
    "        input_layer = keras.Input(shape=(n_steps,), name='input_nn')\n",
    "        x = layers.Layer(trainable = False, name = 'adhoc_structure_layer')(input_layer)\n",
    "        num_layer = 0\n",
    "        if is_sequential:\n",
    "            x = layers.GRU(intermediate_dim, name = 'sequential_layer')(x)\n",
    "            num_layer += 1\n",
    "        output_layer = layers.Dense(units = n_steps, activation = 'relu', name = 'first_dense')(x)\n",
    "        self._custom_model = model = keras.Model(\n",
    "            inputs=[input_layer],\n",
    "            outputs=[output_layer],\n",
    "        )\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self._grads = None\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._custom_model\n",
    "\n",
    "    # Function to get dV/dX after each epoch\n",
    "    # TODO: Change into a function that returns the grads per variable and not all the grads.\n",
    "    def get_dv_dx(self, features):\n",
    "        xs = tf.Variable(features)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = self._custom_model(xs)\n",
    "        # This represents dV/dX\n",
    "        self._grads = tape.gradient(y, xs)\n",
    "        return self._grads\n",
    "    \n",
    "    def get_dv_dxi(self, i, sample_idx = 0):\n",
    "        print(i)\n",
    "        return self._grads[sample_idx][i] if self._grads is not None else None\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self._custom_model(inputs)\n",
    "    \n",
    "    # Path prediction\n",
    "    # TODO: Vectorize for simultaneous multiple paths\n",
    "    # TODO: Check\n",
    "    def predict_path(self, x, sample_idx = 0):\n",
    "        # Steps\n",
    "        N = x.shape[1]\n",
    "        # X path\n",
    "        x_path = x[sample_idx]\n",
    "        # Swaping option at strike\n",
    "        v = np.zeros((1,N))[0]\n",
    "        predictions = self._custom_model(x).numpy()[0]\n",
    "        # Keep only the first value predicted\n",
    "        v[0] = predictions[0]\n",
    "        # Get the gradients\n",
    "        grads = self.get_dv_dx(x).numpy()[sample_idx]\n",
    "        # Do the iterative process\n",
    "        for i in range(0, N - 1):\n",
    "            v[i + 1] = v[i] + grads[i] * (x_path[i + 1] - x_path[i])\n",
    "        \n",
    "        return v, predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative process\n",
    "* F - neural network function.\n",
    "* $\\frac{\\delta F}{\\delta X_t}^i$ - gradient calculated by using the model at $i$-iteration.\n",
    "* $\\phi(n, x_n)$ - known terminal function.\n",
    "\n",
    "\n",
    "**Path generation**\n",
    "$$\\hat{V} = F(X)$$\n",
    "\n",
    "$$\\overline{V}_0 = \\hat{V}^i[0]$$\n",
    "\n",
    "$$\\overline{V}_{t+1} = \\overline{V}_t + \\frac{\\delta F(X)}{\\delta x_t}(x_{t + 1} - x_{t})$$\n",
    "\n",
    "**Loss function**\n",
    "$$\\mathcal{L}(\\overline{V}, \\hat{V}) = \\beta_1 \\cdot (\\hat{V}_n - \\phi(n, x_n))^2 + \\beta_2\\cdot (\\hat{V}_n - \\frac{\\delta F(X)}{\\delta x_n})^2 + \\sum_{i = 1}^{n - 1}(\\overline{V}_i - \\hat{V}_i)^2  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f(xn, n):\n",
    "  return xn ** 2 + n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change np.arrays to tf.Tensor\n",
    "# TODO2: Include better method descriptions\n",
    "def custom_loss_lgm(x = np.array, path = np.array, predictions = np.array):\n",
    "    ''' \n",
    "    Beta:\n",
    "        * Beta[0] - error related to predictions\n",
    "        * Beta[1] - error related to strike\n",
    "        * Beta[2] - error related to derivative at strike\n",
    "    '''\n",
    "    betas = [1.0, 0.5, 0.5]\n",
    "    # Careful: Using global variable...\n",
    "    len_path = N\n",
    "    # For f and f'\n",
    "    xn = tf.Variable(x[0,-1], name = 'xn')\n",
    "    n = tf.Variable(np.float64(len_path), name = 'n')\n",
    "    n_idx = int(len_path)\n",
    "    # Loss given the strike function\n",
    "    strike_loss = (x[-1] - f(xn, n))**2\n",
    "    # Autodiff f\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(xn, n)\n",
    "    grad_df = tape.gradient(y, {\n",
    "        'xn':xn,\n",
    "        'n':n    \n",
    "    })\n",
    "    df_dxn = grad_df['xn']\n",
    "    # Careful: global variable\n",
    "    derivative_loss = (lgm.get_dv_dxi(n_idx - 1) - df_dxn)**2\n",
    "    # Epoch error per step\n",
    "    error_per_step = np.sum((path[:-1] - predictions[:-1])**2)\n",
    "\n",
    "    return np.dot(np.array(betas), np.array([error_per_step, strike_loss, derivative_loss]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "99\n",
      "Mean loss = 4999.9395\n",
      "Start of epoch 1\n",
      "99\n",
      "Mean loss = 4999.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\borja\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Custom model\n",
    "lgm = LGM_model(N, 64)\n",
    "# Data used as features\n",
    "features = np.reshape(mc_value_per_time_step, (1, N))\n",
    "# Loss metric\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        x = tf.Variable(features, trainable = False)\n",
    "        path = tf.Variable(path, trainable = False)\n",
    "        predictions = tf.Variable(predictions, trainable = False)        \n",
    "        path, predictions = lgm.predict_path(features)\n",
    "        loss = custom_loss_lgm(x = features,  path = path, predictions = predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, lgm.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, lgm.trainable_weights))\n",
    "\n",
    "    loss_metric(loss)\n",
    "    print(\"Mean loss = %.4f\" % (loss_metric.result()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6894ab1ff2938d33f6c65f3e6035a3da8cd591293cf5e338693c371b63dd3711"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
