{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward basic\n",
    "\n",
    "Basic equation with just one layer:\n",
    "\n",
    "$$NN(X) = f( W\\cdot X + b)$$\n",
    "with $f(\\cdot)$ a ReLU function:\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "    0, & \\text{if } x < 0\\\\\n",
    "    x, & \\text{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "and $W$ is a matrix of weights per neuron in the layer with dimension $(|N|_{n+1} \\times |N|_n)$\n",
    "Derivative given the entry:\n",
    "$$\n",
    "\\frac{dNN_j(X)}{dX_i} = \\frac{dNN_j}{df}\\frac{df}{dX_i} = \n",
    "\\begin{cases}\n",
    "    0, & \\text{if } x < 0\\\\\n",
    "    W_{ji}, & \\text{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "* Given a neural network with 1 layer and 2 neurons in the layer we have:\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "    1 & 2 & 3\\\\\n",
    "    3 & 4 & 6\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    2 \\\\\n",
    "    3\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 19:15:49.853242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Custom utils\n",
    "from utils.simulator.simulator import MCSimulation\n",
    "# Tf imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    return tf.keras.activations.relu(X).numpy()\n",
    "\n",
    "def relu_derivative(X, partial_x = 0, partial_j = 0):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    return X[partial_j][partial_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.Input(shape = (4,), name='input_nn')\n",
    "dense_unit = layers.Dense(\n",
    "    units = 2,\n",
    "    activation = 'relu',\n",
    "    name = 'dense_layer'\n",
    ")(input_layer)\n",
    "custom_model = keras.Model(\n",
    "    inputs=[input_layer],\n",
    "    outputs=[dense_unit],\n",
    "    name = 'test_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los pesos de la capa oculta\n",
    "w = custom_model.get_layer('dense_layer').get_weights()[0].T\n",
    "b = custom_model.get_layer('dense_layer').get_weights()[1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor(\n",
    "    np.array(range(-13,-1)).reshape((4,3)),\n",
    "    dtype = tf.float32    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_result = tf.transpose(relu(tf.matmul(w, x)))\n",
    "model_output = custom_model(tf.transpose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 2.114374 , 12.523226 ],\n",
       "        [ 1.0487409, 11.261716 ],\n",
       "        [ 0.       , 10.000206 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 2.114374 , 12.523226 ],\n",
       "        [ 1.0487409, 11.261716 ],\n",
       "        [ 0.       , 10.000206 ]], dtype=float32)>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_result, model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f8f5a557e20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "xs = tf.Variable(x, trainable = True, name = 'x')\n",
    "with tf.GradientTape() as tape, tf.GradientTape() as tape_2:\n",
    "    tape.watch(xs)\n",
    "    tape_2.watch(xs)\n",
    "    y = custom_model(tf.transpose(xs))\n",
    "# This represents dV/dX\n",
    "grads = tape.gradient(y, {\n",
    "    'x':xs\n",
    "})\n",
    "jacobian = tape_2.jacobian(y, {\n",
    "    'x':xs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[-0.25807047, -0.25807047, -0.8092563 ],\n",
       "       [-0.13669562, -0.13669562, -0.00675702],\n",
       "       [-0.72874   , -0.72874   , -0.05111217],\n",
       "       [-1.2036369 , -1.2036369 , -0.39438462]], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 4, 3), dtype=float32, numpy=\n",
       "array([[[[ 0.55118585,  0.        ,  0.        ],\n",
       "         [-0.1299386 ,  0.        ,  0.        ],\n",
       "         [-0.6776278 ,  0.        ,  0.        ],\n",
       "         [-0.80925226,  0.        ,  0.        ]],\n",
       "\n",
       "        [[-0.8092563 ,  0.        ,  0.        ],\n",
       "         [-0.00675702,  0.        ,  0.        ],\n",
       "         [-0.05111217,  0.        ,  0.        ],\n",
       "         [-0.39438462,  0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.55118585,  0.        ],\n",
       "         [ 0.        , -0.1299386 ,  0.        ],\n",
       "         [ 0.        , -0.6776278 ,  0.        ],\n",
       "         [ 0.        , -0.80925226,  0.        ]],\n",
       "\n",
       "        [[ 0.        , -0.8092563 ,  0.        ],\n",
       "         [ 0.        , -0.00675702,  0.        ],\n",
       "         [ 0.        , -0.05111217,  0.        ],\n",
       "         [ 0.        , -0.39438462,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        , -0.8092563 ],\n",
       "         [ 0.        ,  0.        , -0.00675702],\n",
       "         [ 0.        ,  0.        , -0.05111217],\n",
       "         [ 0.        ,  0.        , -0.39438462]]]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_grads = np.zeros((\n",
    "    2,4\n",
    "))\n",
    "for j in range(2):\n",
    "    for i in range(4):\n",
    "        custom_grads[j,i] = relu_derivative(w, partial_x = i, partial_j = j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55118585, -0.1299386 , -0.6776278 , -0.80925226],\n",
       "       [-0.80925632, -0.00675702, -0.05111217, -0.39438462]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b391e97e35ec4987120a2f780cd64183fdb56c026e5a7e01d3347b3d6528b2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
